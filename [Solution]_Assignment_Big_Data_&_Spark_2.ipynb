{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hc4RCb5aSLRn",
        "09tFpzozSQOb",
        "Ml31Nz9vSVcu"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arutraj/.githubcl/blob/main/%5BSolution%5D_Assignment_Big_Data_%26_Spark_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install pyspark and customize Colab configuration\n",
        "\n",
        "\n",
        "The pyspark installation will persist until the runtime is recycled."
      ],
      "metadata": {
        "id": "hc4RCb5aSLRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x94ufk58SBs_",
        "outputId": "8822b622-e2bf-48a2-ebcd-da7d83e45cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Connecting to security.ubuntu.com (91.189.91.83)] [Connected to cloud.r-pro\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\u001b[33m\r0% [2 InRelease 15.6 kB/128 kB 12%] [Connecting to security.ubuntu.com (91.189.\u001b[0m\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [2 InRelease 25.8 kB/128 kB 20%] [Connecting to security.ubuntu.com (91.189.\u001b[0m\u001b[33m\r0% [2 InRelease 25.8 kB/128 kB 20%] [Connecting to security.ubuntu.com (91.189.\u001b[0m\u001b[33m\r0% [Connecting to security.ubuntu.com (91.189.91.83)] [Connecting to r2u.stat.i\u001b[0m\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "\u001b[33m\r0% [4 InRelease 15.6 kB/127 kB 12%] [Waiting for headers] [Connecting to r2u.st\u001b[0m\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,665 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,105 kB]\n",
            "Ign:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,436 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,606 kB]\n",
            "Fetched 15.2 MB in 3s (5,748 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "!pip install ucimlrepo\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Spark"
      ],
      "metadata": {
        "id": "09tFpzozSQOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "spark"
      ],
      "metadata": {
        "id": "O0gRmAjaSNxn",
        "outputId": "8200e95f-6449-4c5d-f6ca-e6e3899a2b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a2365f81a50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7ae18d6d300b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Bank Dataset"
      ],
      "metadata": {
        "id": "Ml31Nz9vSVcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "bank_marketing = fetch_ucirepo(id=222)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "dataset_X = bank_marketing.data.features\n",
        "dataset_y = bank_marketing.data.targets\n",
        "\n",
        "dataset = pd.concat([dataset_X, dataset_y], axis=1)\n",
        "dataset.rename(columns={\"y\": \"deposit\"}, inplace=True)\n",
        "\n",
        "sdf = spark.createDataFrame(dataset)\n",
        "sdf.createOrReplaceTempView(\"spktest_bank\")"
      ],
      "metadata": {
        "id": "Yo2RZc0wSSmA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions"
      ],
      "metadata": {
        "id": "E7tJQHrwS4KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.show()"
      ],
      "metadata": {
        "id": "fNioQ9JPTA_9",
        "outputId": "c55bdf09-e08b-4a11-f970-c78d99b1f76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+--------+---------+-------+-------+-------+----+-------+-----------+-----+--------+--------+-----+--------+--------+-------+\n",
            "|age|         job| marital|education|default|balance|housing|loan|contact|day_of_week|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+-------+-----------+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 58|  management| married| tertiary|     no|   2143|    yes|  no|    NaN|          5|  may|     261|       1|   -1|       0|     NaN|     no|\n",
            "| 44|  technician|  single|secondary|     no|     29|    yes|  no|    NaN|          5|  may|     151|       1|   -1|       0|     NaN|     no|\n",
            "| 33|entrepreneur| married|secondary|     no|      2|    yes| yes|    NaN|          5|  may|      76|       1|   -1|       0|     NaN|     no|\n",
            "| 47| blue-collar| married|      NaN|     no|   1506|    yes|  no|    NaN|          5|  may|      92|       1|   -1|       0|     NaN|     no|\n",
            "| 33|         NaN|  single|      NaN|     no|      1|     no|  no|    NaN|          5|  may|     198|       1|   -1|       0|     NaN|     no|\n",
            "| 35|  management| married| tertiary|     no|    231|    yes|  no|    NaN|          5|  may|     139|       1|   -1|       0|     NaN|     no|\n",
            "| 28|  management|  single| tertiary|     no|    447|    yes| yes|    NaN|          5|  may|     217|       1|   -1|       0|     NaN|     no|\n",
            "| 42|entrepreneur|divorced| tertiary|    yes|      2|    yes|  no|    NaN|          5|  may|     380|       1|   -1|       0|     NaN|     no|\n",
            "| 58|     retired| married|  primary|     no|    121|    yes|  no|    NaN|          5|  may|      50|       1|   -1|       0|     NaN|     no|\n",
            "| 43|  technician|  single|secondary|     no|    593|    yes|  no|    NaN|          5|  may|      55|       1|   -1|       0|     NaN|     no|\n",
            "| 41|      admin.|divorced|secondary|     no|    270|    yes|  no|    NaN|          5|  may|     222|       1|   -1|       0|     NaN|     no|\n",
            "| 29|      admin.|  single|secondary|     no|    390|    yes|  no|    NaN|          5|  may|     137|       1|   -1|       0|     NaN|     no|\n",
            "| 53|  technician| married|secondary|     no|      6|    yes|  no|    NaN|          5|  may|     517|       1|   -1|       0|     NaN|     no|\n",
            "| 58|  technician| married|      NaN|     no|     71|    yes|  no|    NaN|          5|  may|      71|       1|   -1|       0|     NaN|     no|\n",
            "| 57|    services| married|secondary|     no|    162|    yes|  no|    NaN|          5|  may|     174|       1|   -1|       0|     NaN|     no|\n",
            "| 51|     retired| married|  primary|     no|    229|    yes|  no|    NaN|          5|  may|     353|       1|   -1|       0|     NaN|     no|\n",
            "| 45|      admin.|  single|      NaN|     no|     13|    yes|  no|    NaN|          5|  may|      98|       1|   -1|       0|     NaN|     no|\n",
            "| 57| blue-collar| married|  primary|     no|     52|    yes|  no|    NaN|          5|  may|      38|       1|   -1|       0|     NaN|     no|\n",
            "| 60|     retired| married|  primary|     no|     60|    yes|  no|    NaN|          5|  may|     219|       1|   -1|       0|     NaN|     no|\n",
            "| 33|    services| married|secondary|     no|      0|    yes|  no|    NaN|          5|  may|      54|       1|   -1|       0|     NaN|     no|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+-------+-----------+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM spktest_bank\").show()"
      ],
      "metadata": {
        "id": "x1GX7mF4e03u",
        "outputId": "4130c128-4508-4690-b3ef-d99b434b68f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+--------+---------+-------+-------+-------+----+-------+-----------+-----+--------+--------+-----+--------+--------+-------+\n",
            "|age|         job| marital|education|default|balance|housing|loan|contact|day_of_week|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+-------+-----------+-----+--------+--------+-----+--------+--------+-------+\n",
            "| 58|  management| married| tertiary|     no|   2143|    yes|  no|    NaN|          5|  may|     261|       1|   -1|       0|     NaN|     no|\n",
            "| 44|  technician|  single|secondary|     no|     29|    yes|  no|    NaN|          5|  may|     151|       1|   -1|       0|     NaN|     no|\n",
            "| 33|entrepreneur| married|secondary|     no|      2|    yes| yes|    NaN|          5|  may|      76|       1|   -1|       0|     NaN|     no|\n",
            "| 47| blue-collar| married|      NaN|     no|   1506|    yes|  no|    NaN|          5|  may|      92|       1|   -1|       0|     NaN|     no|\n",
            "| 33|         NaN|  single|      NaN|     no|      1|     no|  no|    NaN|          5|  may|     198|       1|   -1|       0|     NaN|     no|\n",
            "| 35|  management| married| tertiary|     no|    231|    yes|  no|    NaN|          5|  may|     139|       1|   -1|       0|     NaN|     no|\n",
            "| 28|  management|  single| tertiary|     no|    447|    yes| yes|    NaN|          5|  may|     217|       1|   -1|       0|     NaN|     no|\n",
            "| 42|entrepreneur|divorced| tertiary|    yes|      2|    yes|  no|    NaN|          5|  may|     380|       1|   -1|       0|     NaN|     no|\n",
            "| 58|     retired| married|  primary|     no|    121|    yes|  no|    NaN|          5|  may|      50|       1|   -1|       0|     NaN|     no|\n",
            "| 43|  technician|  single|secondary|     no|    593|    yes|  no|    NaN|          5|  may|      55|       1|   -1|       0|     NaN|     no|\n",
            "| 41|      admin.|divorced|secondary|     no|    270|    yes|  no|    NaN|          5|  may|     222|       1|   -1|       0|     NaN|     no|\n",
            "| 29|      admin.|  single|secondary|     no|    390|    yes|  no|    NaN|          5|  may|     137|       1|   -1|       0|     NaN|     no|\n",
            "| 53|  technician| married|secondary|     no|      6|    yes|  no|    NaN|          5|  may|     517|       1|   -1|       0|     NaN|     no|\n",
            "| 58|  technician| married|      NaN|     no|     71|    yes|  no|    NaN|          5|  may|      71|       1|   -1|       0|     NaN|     no|\n",
            "| 57|    services| married|secondary|     no|    162|    yes|  no|    NaN|          5|  may|     174|       1|   -1|       0|     NaN|     no|\n",
            "| 51|     retired| married|  primary|     no|    229|    yes|  no|    NaN|          5|  may|     353|       1|   -1|       0|     NaN|     no|\n",
            "| 45|      admin.|  single|      NaN|     no|     13|    yes|  no|    NaN|          5|  may|      98|       1|   -1|       0|     NaN|     no|\n",
            "| 57| blue-collar| married|  primary|     no|     52|    yes|  no|    NaN|          5|  may|      38|       1|   -1|       0|     NaN|     no|\n",
            "| 60|     retired| married|  primary|     no|     60|    yes|  no|    NaN|          5|  may|     219|       1|   -1|       0|     NaN|     no|\n",
            "| 33|    services| married|secondary|     no|      0|    yes|  no|    NaN|          5|  may|      54|       1|   -1|       0|     NaN|     no|\n",
            "+---+------------+--------+---------+-------+-------+-------+----+-------+-----------+-----+--------+--------+-----+--------+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Using Spark SQL, find marital status and corresponding total count for all the people."
      ],
      "metadata": {
        "id": "sIi1WqeqS5IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "spark.sql(\"SELECT marital, COUNT(*) AS num_people FROM spktest_bank GROUP BY marital\").show()"
      ],
      "metadata": {
        "id": "qtsAfJgWTRof",
        "outputId": "f00e38d2-1a73-42ed-b3ae-f9a8a2f70ab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "| marital|num_people|\n",
            "+--------+----------+\n",
            "|divorced|      5207|\n",
            "| married|     27214|\n",
            "|  single|     12790|\n",
            "+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Find number of people with balance greater than or equal to 80<sup>th</sup> percentile."
      ],
      "metadata": {
        "id": "31tytXXbTUFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "spark.sql(\"SELECT COUNT(*) AS num_people FROM spktest_bank WHERE balance < (SELECT PERCENTILE(balance, 0.75) FROM spktest_bank)\").show()"
      ],
      "metadata": {
        "id": "tlr6wEdjTS9P",
        "outputId": "301e5a30-a243-4aef-ae3c-4a0f614a158b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|num_people|\n",
            "+----------+\n",
            "|     33907|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Find the Average Balance of clients who made a deposit."
      ],
      "metadata": {
        "id": "w0rfwsfqTY8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "spark.sql(\"SELECT AVG(balance) AS avg_balance FROM spktest_bank GROUP BY deposit HAVING deposit='yes'\").show()"
      ],
      "metadata": {
        "id": "SXf-nHiQTYdS",
        "outputId": "7de95f6b-441e-4b89-f5c5-8254e036fe48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+\n",
            "|       avg_balance|\n",
            "+------------------+\n",
            "|1804.2679145396105|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Spark SQL query using Common Table Expressions (CTEs) and LIMIT clause to find the job category with the highest average balance among clients who have not defaulted on their loans and have made a deposit. The query should consider only clients aged between 30 and 50."
      ],
      "metadata": {
        "id": "sLsrZjaTTfXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "spark.sql(\"\"\"\n",
        "  WITH clients AS (\n",
        "    SELECT * FROM spktest_bank WHERE default='no' and deposit='yes' and age BETWEEN 30 AND 50\n",
        "  )\n",
        "\n",
        "  SELECT job, AVG(balance) AS avg_balance FROM clients GROUP BY job ORDER BY avg_balance DESC LIMIT 3\n",
        "  \"\"\").show()"
      ],
      "metadata": {
        "id": "-ViT__7FTewU",
        "outputId": "d78237f9-df32-4794-8857-3011aa683f8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|       job|       avg_balance|\n",
            "+----------+------------------+\n",
            "|   retired|3030.1666666666665|\n",
            "|management|2099.8103837471785|\n",
            "| housemaid|           2034.82|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Spark SQL query using CTE and WINDOW function to find the job category with the highest average balance among clients who have not defaulted on their loans and have made a deposit. The query should consider only clients aged between 30 and 50."
      ],
      "metadata": {
        "id": "2dIb6s5QTf3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "spark.sql(\"\"\"\n",
        "  WITH clients AS (\n",
        "    SELECT * FROM spktest_bank WHERE default='no' and deposit='yes' and age BETWEEN 30 AND 50\n",
        "  ),\n",
        "\n",
        "  avg_bal AS (\n",
        "    SELECT job, AVG(balance) AS avg_balance FROM clients GROUP BY job\n",
        "  )\n",
        "\n",
        "  SELECT job, avg_balance, rank\n",
        "  FROM\n",
        "    (SELECT *, ROW_NUMBER() OVER (ORDER BY avg_balance DESC) AS rank FROM avg_bal)\n",
        "  WHERE rank < 6\n",
        "  \"\"\").show()"
      ],
      "metadata": {
        "id": "ZI-XwJiOT3-h",
        "outputId": "a2a31462-ccf2-455f-f520-5bc79214fba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------------+----+\n",
            "|          job|       avg_balance|rank|\n",
            "+-------------+------------------+----+\n",
            "|      retired|3030.1666666666665|   1|\n",
            "|   management|2099.8103837471785|   2|\n",
            "|    housemaid|           2034.82|   3|\n",
            "|   technician| 1923.380357142857|   4|\n",
            "|self-employed|         1805.6875|   5|\n",
            "+-------------+------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Spark SQL query to find the month with the highest number of clients who have made a deposit and have not defaulted on their loans."
      ],
      "metadata": {
        "id": "YYbxtTa3TgTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "spark.sql(\"SELECT month, COUNT(*) AS num_clients FROM spktest_bank WHERE default='no' AND deposit='yes' GROUP BY month ORDER BY num_clients DESC LIMIT 2\").show()"
      ],
      "metadata": {
        "id": "qFqld8LyT8ag",
        "outputId": "3ef9e635-03e6-4452-95c1-a875864011e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+\n",
            "|month|num_clients|\n",
            "+-----+-----------+\n",
            "|  may|        917|\n",
            "|  aug|        683|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Spark SQL query using an User Defined Function(UDF) to calculate average duration in weeks for each job type?"
      ],
      "metadata": {
        "id": "kDrrwfzqT-dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "def get_duration_in_weeks(days):\n",
        "  return days/7\n",
        "\n",
        "spark.udf.register(\"get_duration_in_weeks\", get_duration_in_weeks, T.DoubleType())\n",
        "\n",
        "spark.sql(\"SELECT job, get_duration_in_weeks(AVG(duration)) AS avg_duration_in_weeks FROM spktest_bank GROUP BY job order by avg_duration_in_weeks desc\").show()"
      ],
      "metadata": {
        "id": "kzPR1yX5T9ON",
        "outputId": "b3184879-c1a6-40c7-c804-68122a019fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------------+\n",
            "|          job|avg_duration_in_weeks|\n",
            "+-------------+---------------------+\n",
            "|   unemployed|    41.22048021050323|\n",
            "|      retired|    41.05161534578496|\n",
            "|self-employed|     38.3081516330408|\n",
            "|  blue-collar|    37.55736597968411|\n",
            "|     services|    37.04553270513791|\n",
            "| entrepreneur|    36.61562109712749|\n",
            "|   management|   36.285110110866086|\n",
            "|   technician|    36.12928035502736|\n",
            "|       admin.|    35.27096168190734|\n",
            "|      student|    35.23667377398721|\n",
            "|    housemaid|    35.11785714285714|\n",
            "|          NaN|    33.94444444444444|\n",
            "+-------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Using CTE and JOIN operator, write a Spark SQL query to find the Median Duration and Median Balance for each job type."
      ],
      "metadata": {
        "id": "i3hCuBeUUAPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "spark.sql(\"\"\"\n",
        "  WITH median_duration_cte AS (\n",
        "    SELECT job, PERCENTILE(duration, 0.5) AS median_duration FROM spktest_bank GROUP BY job\n",
        "  ),\n",
        "\n",
        "  median_balance_cte AS (\n",
        "    SELECT job, PERCENTILE(balance, 0.5) AS median_balance FROM bank GROUP BY job\n",
        "  )\n",
        "\n",
        "  SELECT\n",
        "    median_duration_cte.job,\n",
        "    median_duration,\n",
        "    median_balance\n",
        "  FROM\n",
        "    median_duration_cte\n",
        "  JOIN\n",
        "    median_balance_cte\n",
        "  ON median_duration_cte.job = median_balance_cte.job\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "eg-uWPGPT92g",
        "outputId": "5f6bc51d-96a2-4412-f34d-ceced74f4935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------+--------------+\n",
            "|          job|median_duration|median_balance|\n",
            "+-------------+---------------+--------------+\n",
            "|   management|          173.0|         572.0|\n",
            "|      retired|          204.0|         787.0|\n",
            "|self-employed|          179.0|         526.0|\n",
            "|      student|          180.0|         502.0|\n",
            "|  blue-collar|          186.0|         388.0|\n",
            "| entrepreneur|          178.0|         352.0|\n",
            "|       admin.|          174.0|         396.0|\n",
            "|   technician|          176.0|         421.0|\n",
            "|          NaN|          165.0|         677.0|\n",
            "|     services|          186.0|         339.5|\n",
            "|    housemaid|          163.0|         406.0|\n",
            "|   unemployed|          200.0|         529.0|\n",
            "+-------------+---------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Using Statistic package from MLlib, find the Pearson correlation between Balance and Duration."
      ],
      "metadata": {
        "id": "TG6_gP0JUA7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "from pyspark.mllib.stat import Statistics\n",
        "\n",
        "df_x = spark.sql(\"SELECT balance, duration FROM spktest_bank\").rdd.map(lambda x: x[0])\n",
        "df_y = spark.sql(\"SELECT duration FROM spktest_bank\").rdd.map(lambda x: x[0])\n",
        "\n",
        "print(df_x, df_y)\n",
        "print(\"Pearson Correlation: \", Statistics.corr(df_x, df_y, method='pearson'))"
      ],
      "metadata": {
        "id": "YZaF-m2GT-Ho",
        "outputId": "50d1150e-628c-48d5-d691-d8483a33feb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[249] at RDD at PythonRDD.scala:53 PythonRDD[250] at RDD at PythonRDD.scala:53\n",
            "Pearson Correlation:  0.02156038049466903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Build a k-means model with k=2 clusters on a balanced dataset and evaluate the clusters using Silhouette coefficient."
      ],
      "metadata": {
        "id": "lICXSC6sUBpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "\n",
        "# Stratified sampling to balance the dataset\n",
        "fractions = {\n",
        "    \"no\": 0.10,\n",
        "    \"yes\": 1.0\n",
        "}\n",
        "\n",
        "sdf = sdf.sampleBy(\"deposit\", fractions, seed=212)\n",
        "sdf = sdf.select(\n",
        "    'age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan',\n",
        "    'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit'\n",
        ")\n",
        "cols = sdf.columns\n",
        "sdf.printSchema()"
      ],
      "metadata": {
        "id": "qq3skjHBUCOZ",
        "outputId": "717098ca-82ae-4cec-82a7-bd04a322c2f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: long (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- duration: long (nullable = true)\n",
            " |-- campaign: long (nullable = true)\n",
            " |-- pdays: long (nullable = true)\n",
            " |-- previous: long (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
        "\n",
        "stages = []\n",
        "categoricalColumns = [\n",
        "    'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome'\n",
        "]\n",
        "\n",
        "for categoricalCol in categoricalColumns:\n",
        "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
        "    encoder = OneHotEncoder(\n",
        "        inputCols=[stringIndexer.getOutputCol()],\n",
        "        outputCols=[categoricalCol + \"classVec\"]\n",
        "    )\n",
        "    stages += [stringIndexer, encoder]\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
        "stages += [label_stringIdx]\n",
        "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
        "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
        "stages += [assembler]"
      ],
      "metadata": {
        "id": "hlBYrwXyuo4n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages = stages)\n",
        "pipelineModel = pipeline.fit(sdf)\n",
        "sdf = pipelineModel.transform(sdf)\n",
        "selectedCols = ['label', 'features'] + cols\n",
        "sdf = sdf.select(selectedCols)\n",
        "sdf.printSchema()"
      ],
      "metadata": {
        "id": "wrS2UIl3urMA",
        "outputId": "75e14db6-dc2e-4508-e260-4064df1efcdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- label: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: long (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- duration: long (nullable = true)\n",
            " |-- campaign: long (nullable = true)\n",
            " |-- pdays: long (nullable = true)\n",
            " |-- previous: long (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans(featuresCol = 'features', k=2, seed=1)\n",
        "model = kmeans.fit(sdf)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(sdf)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "metadata": {
        "id": "4esDdGWpvia-",
        "outputId": "a505bff3-d237-4bf5-cbe4-977268556f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette with squared euclidean distance = 0.9760083517189627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7e9HkReeUC1p"
      }
    }
  ]
}