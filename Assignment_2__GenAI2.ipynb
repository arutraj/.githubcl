{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arutraj/.githubcl/blob/main/Assignment_2__GenAI2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT: **It is highly recommended that this notebook be run on Google Colab itself as it contains CUDA usage, which may not be supported in the local environment of some computers."
      ],
      "metadata": {
        "id": "A5lSxRvmdrqy"
      },
      "id": "A5lSxRvmdrqy"
    },
    {
      "cell_type": "markdown",
      "id": "2b949f9f",
      "metadata": {
        "id": "2b949f9f"
      },
      "source": [
        "# BLIP: Inference Demo\n",
        " - [Image Captioning](#Image-Captioning)\n",
        " - [VQA](#VQA)\n",
        " - [Feature Extraction](#Feature-Extraction)\n",
        " - [Image Text Matching](#Image-Text-Matching)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cbcb066b",
      "metadata": {
        "id": "cbcb066b",
        "outputId": "97f0734f-6a94-4961-c45d-80b758fdc2e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Collecting transformers==4.16.0\n",
            "  Downloading transformers-4.16.0-py3-none-any.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting fairscale==0.4.4\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (2.32.3)\n",
            "Collecting sacremoses (from transformers==4.16.0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.16.0) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.19.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.16.0) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.0) (1.4.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.12) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n",
            "Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292833 sha256=dcebf736c9f2d971b1a8e62e24a7c743fbfc265850e96958b99e3d0e92ea8411\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/58/6f/56c57fa8315eb0bcf0287b580c850845be5f116359b809e9f1\n",
            "Successfully built fairscale\n",
            "Installing collected packages: sacremoses, fairscale, transformers, timm\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed fairscale-0.4.4 sacremoses-0.1.1 timm-0.4.12 transformers-4.16.0\n",
            "Cloning into 'BLIP'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 277 (delta 145), reused 137 (delta 137), pack-reused 94 (from 1)\u001b[K\n",
            "Receiving objects: 100% (277/277), 7.04 MiB | 15.07 MiB/s, done.\n",
            "Resolving deltas: 100% (152/152), done.\n",
            "/content/BLIP\n"
          ]
        }
      ],
      "source": [
        "# Installing the requirements\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !pip3 install transformers==4.16.0 timm==0.4.12 fairscale==0.4.4\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd BLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a811a65f",
      "metadata": {
        "id": "a811a65f"
      },
      "outputs": [],
      "source": [
        "#Importing the required modules\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "#Setting the device based on hardware availability to mps or cuda otherwise set it to cpu for PyTorch operations\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Transforming and loading the image\n",
        "def load_demo_image(image_size,device):\n",
        "    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
        "    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "\n",
        "    w,h = raw_image.size\n",
        "    display(raw_image.resize((w//5,h//5)))\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "    image = transform(raw_image).unsqueeze(0).to(device)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72f4406",
      "metadata": {
        "id": "f72f4406"
      },
      "source": [
        "# **Question 1: Image Captioning**\n",
        "\n",
        "## Perform image captioning using the BLIP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6835daef",
      "metadata": {
        "id": "6835daef"
      },
      "outputs": [],
      "source": [
        "#Import the BLIP decoder\n",
        "\n",
        "\n",
        "#Load the image of specific size\n",
        "\n",
        "\n",
        "#Load a pre-trained model and configure it for evaluation\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
        "\n",
        "\n",
        "\n",
        "#Disabling gradient calculations, generate caption either using beam search or by using nucleus sampling and print the caption\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac320a2",
      "metadata": {
        "id": "fac320a2"
      },
      "source": [
        "# **Question 2: Visual Question Answering**\n",
        "## Perform visual question answering using the BLIP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5e6f3fb1",
      "metadata": {
        "id": "5e6f3fb1"
      },
      "outputs": [],
      "source": [
        "#Import the BLIP model for visual question answering\n",
        "\n",
        "\n",
        "#Load the image of specific size\n",
        "\n",
        "\n",
        "#Load a pre-trained model and configure it for evaluation\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'\n",
        "\n",
        "\n",
        "\n",
        "#Frame the question\n",
        "\n",
        "\n",
        "#Disabling gradient calculations, generate answer for the provided question and print the answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6100e519",
      "metadata": {
        "id": "6100e519"
      },
      "source": [
        "# **Question 3: Feature Extraction**\n",
        "\n",
        "## Extract the multimodal, image and text features from the given image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4f8f21ed",
      "metadata": {
        "id": "4f8f21ed",
        "outputId": "441699a4-5029-475f-d838-f26d09801009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 13) (<ipython-input-5-21238ff6887f>, line 13)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-21238ff6887f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    '\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 13)\n"
          ]
        }
      ],
      "source": [
        "#Import the BLIP feature extractor\n",
        "\n",
        "\n",
        "#Load the image of specific size\n",
        "\n",
        "\n",
        "#Load a pre-trained model and configuring it for evaluation\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth'\n",
        "\n",
        "\n",
        "\n",
        "#Frame the caption\n",
        "'\n",
        "\n",
        "#Extract the multimodal, image and text features from the given image and caption using the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "201e1146",
      "metadata": {
        "id": "201e1146"
      },
      "source": [
        "# **Question 4: Image-Text Matching**\n",
        "\n",
        "## Perform an image text matching and compute the similarity between the features and the caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ba5906",
      "metadata": {
        "id": "49ba5906"
      },
      "outputs": [],
      "source": [
        "#Import the BLIP feature extractor\n",
        "\n",
        "\n",
        "#Load the image of specific size\n",
        "\n",
        "\n",
        "\n",
        "#Load a pre-trained model and configuring it for evaluation - Set the device to 'cpu'\n",
        "model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
        "\n",
        "\n",
        "\n",
        "#Frame the caption and print it\n",
        "\n",
        "\n",
        "\n",
        "#Perform an image text matching using the model and print the probability of the likelihood of the match\n",
        "\n",
        "\n",
        "\n",
        "#Compute the cosine similarity between features of an image and the caption, and print it\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}